{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_v3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwNYYqaNfBT2J3Y2v4sif3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aapoorv-tf/abs-summarizer-using-seq2seq/blob/main/seq2seq_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od-ky0V6B30C",
        "outputId": "3bdaedd4-9e15-4465-8efd-d278a68d6343"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XOZekL2B7Qp"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/IR Project')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trz9u48ACD49"
      },
      "source": [
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re           \n",
        "from bs4 import BeautifulSoup \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow_datasets as tfds\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t89kvo9UCFoA",
        "outputId": "bdca0921-27e3-4f3b-abe0-eaffba6cd53c"
      },
      "source": [
        "import tensorflow as tf\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime')\n",
        " \n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.92.3.210:8470']\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.92.3.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.92.3.210:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyj5ATJlCJgv",
        "outputId": "c2a56dff-db07-4102-826e-fb70de9b8c79"
      },
      "source": [
        "!unzip \"/content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/news_summary.zip\" -d \"/content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/news_summary.zip\n",
            "  inflating: /content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/news_summary.csv  \n",
            "  inflating: /content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/news_summary_more.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCQuVE1BCd5-"
      },
      "source": [
        "d1 = pd.read_csv(\"/content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/news_summary_more.csv\", encoding='iso-8859-1')\n",
        "d2 = pd.read_csv(\"/content/gdrive/MyDrive/IR Project/Dataset/SmallDataset/news_summary.csv\", encoding='iso-8859-1')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "YYVS_YFfDMa9",
        "outputId": "d54f9840-a61c-40e5-a5f1-3d50ea851248"
      },
      "source": [
        "d1.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-match winning streak</td>\n",
              "      <td>New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's capt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aegon life iTerm insurance plan helps customers save tax</td>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claims are not true: Sonam</td>\n",
              "      <td>Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             headlines                                                                                                                                                                                                     text\n",
              "0    upGrad learner switches to career in ML & Al with 90% salary hike  Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program ...\n",
              "1         Delhi techie wins free food from Swiggy for one year on CRED  Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coi...\n",
              "2     New Zealand end Rohit Sharma-led India's 12-match winning streak  New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's capt...\n",
              "3             Aegon life iTerm insurance plan helps customers save tax  With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, c...\n",
              "4  Have known Hirani for yrs, what if MeToo claims are not true: Sonam  Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"I..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lngysgxHDZm1"
      },
      "source": [
        "pre1 = d1.iloc[:, 0:2].copy()\n",
        "pre2 = d2.iloc[:, 0:6].copy()\n",
        "\n",
        "pre2['text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep=' '), sep=' '), sep=' '), sep=' ')\n",
        "\n",
        "data = pd.DataFrame()\n",
        "data['article'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
        "data['summary'] = pd.concat([pre1['headlines'], pre2['headlines']],ignore_index=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptr772mbGZ9Q",
        "outputId": "f65e26d5-1f96-4121-eacb-8c4ed9836a9d"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 102915 entries, 0 to 102914\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   article  102797 non-null  object\n",
            " 1   summary  102915 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "QHDstZvLGzGZ",
        "outputId": "dca475c8-c6c0-4ae8-efb8-b9119bd64d49"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program ...</td>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al with 90% salary hike</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coi...</td>\n",
              "      <td>Delhi techie wins free food from Swiggy for one year on CRED</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's capt...</td>\n",
              "      <td>New Zealand end Rohit Sharma-led India's 12-match winning streak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, c...</td>\n",
              "      <td>Aegon life iTerm insurance plan helps customers save tax</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"I...</td>\n",
              "      <td>Have known Hirani for yrs, what if MeToo claims are not true: Sonam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                   article                                                              summary\n",
              "0  Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program ...    upGrad learner switches to career in ML & Al with 90% salary hike\n",
              "1  Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coi...         Delhi techie wins free food from Swiggy for one year on CRED\n",
              "2  New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's capt...     New Zealand end Rohit Sharma-led India's 12-match winning streak\n",
              "3  With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, c...             Aegon life iTerm insurance plan helps customers save tax\n",
              "4  Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"I...  Have known Hirani for yrs, what if MeToo claims are not true: Sonam"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QaEpvqMG4am",
        "outputId": "2d4a8474-f8a7-46b0-a377-f93f5cd32129"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr_MncKQG8yk"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def text_cleaner(text):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    return (\" \".join(tokens)).strip()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjWV_XQeG-H5"
      },
      "source": [
        "cleaned_article = []\n",
        "data.article = data.article.astype(str)\n",
        "for t in data['article']:\n",
        "  cleaned_article.append(text_cleaner(t))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bOwMgp_HAcx"
      },
      "source": [
        "def summary_cleaner(text):\n",
        "    newString = re.sub('\"','', text)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    newString = newString.lower()\n",
        "    return newString"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbQAXRZ0HtOq"
      },
      "source": [
        "cleaned_summary = []\n",
        "data.summary = data.summary.astype(str)\n",
        "for t in data['summary']:\n",
        "    cleaned_summary.append(str(summary_cleaner(t)))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nAlhreawIP3F",
        "outputId": "1cc04b22-0ab9-49ab-e6bb-6f74f0ff221f"
      },
      "source": [
        "cleaned_summary[1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'delhi techie wins free food from swiggy for one year on cred'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwMpaUL9Kfv6"
      },
      "source": [
        "data['cleaned_article'] = cleaned_article\n",
        "data['cleaned_summary'] = cleaned_summary"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yct1Kz0oMC6X",
        "outputId": "456a601a-4bb9-45f1-bf1c-fd18281754a4"
      },
      "source": [
        "wc = pd.DataFrame()\n",
        "wc['article_count'] = data['cleaned_article'].apply(lambda x: len(str(x).split()))\n",
        "for i in range(90,100):\n",
        "  var = wc['article_count'].values\n",
        "  var = np.sort(var,axis=None)\n",
        "  print('At {} percentile, word count is {}'.format(i,var[int(len(var)*(float(i)/100))]))\n",
        "print(\"100 percentile - \",var[-1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At 90 percentile, word count is 41\n",
            "At 91 percentile, word count is 41\n",
            "At 92 percentile, word count is 42\n",
            "At 93 percentile, word count is 42\n",
            "At 94 percentile, word count is 43\n",
            "At 95 percentile, word count is 45\n",
            "At 96 percentile, word count is 108\n",
            "At 97 percentile, word count is 173\n",
            "At 98 percentile, word count is 227\n",
            "At 99 percentile, word count is 300\n",
            "100 percentile -  6737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zeEYGCDMFL8",
        "outputId": "27620bb0-2b52-4615-93f7-ec71328210d8"
      },
      "source": [
        "wc = pd.DataFrame()\n",
        "wc['summary_count'] = data['cleaned_summary'].apply(lambda x: len(str(x).split()))\n",
        "for i in range(90,100):\n",
        "  var = wc['summary_count'].values\n",
        "  var = np.sort(var,axis=None)\n",
        "  print('At {} percentile, word count is {}'.format(i,var[int(len(var)*(float(i)/100))]))\n",
        "print(\"100 percentile - \",var[-1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At 90 percentile, word count is 11\n",
            "At 91 percentile, word count is 11\n",
            "At 92 percentile, word count is 12\n",
            "At 93 percentile, word count is 12\n",
            "At 94 percentile, word count is 12\n",
            "At 95 percentile, word count is 12\n",
            "At 96 percentile, word count is 12\n",
            "At 97 percentile, word count is 12\n",
            "At 98 percentile, word count is 13\n",
            "At 99 percentile, word count is 13\n",
            "100 percentile -  17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11O8Tk60MIUy"
      },
      "source": [
        "max_len_art = 100\n",
        "max_len_summ = 17"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5t5_IljMYIM"
      },
      "source": [
        "cleaned_article =np.array(data['cleaned_article'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_article=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_article)):\n",
        "    if(len(cleaned_summary[i].split())<=max_len_summ and len(cleaned_article[i].split())<=max_len_art):\n",
        "        short_article.append(cleaned_article[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'article':short_article,'summary':short_summary})"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbHfyb9gMaiT"
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x: 'sostok '+ x +' eostok')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlNdCEqdMcwW",
        "outputId": "58683775-3e31-490c-de09-1010e5794e00"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98737, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g6IFcshMgdP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(np.array(df['article']),np.array(df['summary']),test_size=0.3,random_state=0,shuffle=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xyXvYI7MqC2",
        "outputId": "54977639-4ab4-4847-b37e-e5ab4c7ae681"
      },
      "source": [
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "#rare words coverage for article\n",
        "thresh=4\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 57.26561672642552\n",
            "Total Coverage of rare words: 2.334921930562173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQnssJ7eM2fK"
      },
      "source": [
        "x_tokenizer = Tokenizer(num_words = tot_cnt-cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "x_train    =   x_tokenizer.texts_to_sequences(x_train) \n",
        "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "x_train    =   pad_sequences(x_train,  maxlen=max_len_art, padding='post') \n",
        "x_val   =   pad_sequences(x_val, maxlen=max_len_art, padding='post')\n",
        "\n",
        "x_voc_size   =  len(x_tokenizer.word_index) +1"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a63Dq6bM4bd",
        "outputId": "ea130904-eb53-49eb-cfff-34f942db5f5d"
      },
      "source": [
        "x_voc_size"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66100"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDzfYhqiM5fD",
        "outputId": "0b3ebd24-fd4e-4397-e1b7-49f5fcac6029"
      },
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "# rare words coverage in summary\n",
        "thresh=4\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 56.9882777276826\n",
            "Total Coverage of rare words: 3.1210110019166093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0RZaltQM7Vn"
      },
      "source": [
        "y_tokenizer = Tokenizer(num_words = tot_cnt - cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "y_train    =   y_tokenizer.texts_to_sequences(y_train) \n",
        "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "y_train    =   pad_sequences(y_train, maxlen=max_len_summ, padding='post')\n",
        "y_val   =   pad_sequences(y_val, maxlen=max_len_summ, padding='post')\n",
        "\n",
        "y_voc_size  =   len(y_tokenizer.word_index) +1"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JML3oOITM9Gd",
        "outputId": "31575e9e-ec10-4dd5-c70e-330d56f2bf0b"
      },
      "source": [
        "y_voc_size"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28835"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiPgagdJM-vo",
        "outputId": "b8bdbad1-9937-4e6a-ede6-0566f98ff953"
      },
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69115, 69115)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNnET70NhUz"
      },
      "source": [
        "#deleting empty rows\n",
        "ind=[]\n",
        "for i in range(len(y_train)):\n",
        "    cnt=0\n",
        "    for j in y_train[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_train=np.delete(y_train,ind, axis=0)\n",
        "x_train=np.delete(x_train,ind, axis=0)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nvrtoFeNmA8"
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdawpaE4NAdC"
      },
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim=300\n",
        "embedding_dim=100\n",
        "\n",
        "def seq2seq():\n",
        "  \n",
        "  global encoder_inputs\n",
        "  global enc_emb\n",
        "  global encoder_outputs\n",
        "  global state_h\n",
        "  global state_c\n",
        "  global decoder_inputs\n",
        "  global dec_emb_layer\n",
        "  global decoder_lstm\n",
        "  global decoder_dense\n",
        "  encoder_inputs = Input(shape=(max_len_art,))\n",
        "  #embedding layer\n",
        "  \n",
        "  enc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs)\n",
        "  #encoder lstm 1\n",
        "  encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "  encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "  #encoder lstm 2\n",
        "  encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "  encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "    #encoder lstm 3\n",
        "  encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "  \n",
        "  encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "  \n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  #embedding layer\n",
        "  \n",
        "  dec_emb_layer = Embedding(y_voc_size, embedding_dim,trainable=True)\n",
        "  dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "  \n",
        "  decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "  decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "  #dense layer\n",
        "  \n",
        "  decoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  return model\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kDodpz6Nos9",
        "outputId": "a31922d3-8f62-40ec-d9b3-98c89221784b"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  model1=seq2seq()\n",
        "model1.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 100)     6610000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 100, 300), ( 481200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 100, 300), ( 721200      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    2883500     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 100, 300), ( 721200      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 28835)  8679335     lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 20,577,635\n",
            "Trainable params: 20,577,635\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUKjbgThNq5Y"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "nadam = optimizers.Nadam(lr=0.0001,\n",
        "                         beta_1=0.9,\n",
        "                         beta_2=0.999,\n",
        "                         epsilon=1e-8, \n",
        "                         schedule_decay=0.004)\n",
        "model1.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=nadam,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApbooZQXNuRA"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7ZCe9dbNvjj",
        "outputId": "69ba9fda-1314-4059-9858-0adb78563597"
      },
      "source": [
        "history=model1.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:],epochs=30,callbacks=[es],batch_size=256, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "270/270 [==============================] - 76s 211ms/step - loss: 5.4028 - acc: 0.3673 - val_loss: 4.8226 - val_acc: 0.3719\n",
            "Epoch 2/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.7650 - acc: 0.3735 - val_loss: 4.6407 - val_acc: 0.3931\n",
            "Epoch 3/30\n",
            "270/270 [==============================] - 50s 183ms/step - loss: 4.5981 - acc: 0.4104 - val_loss: 4.4851 - val_acc: 0.4475\n",
            "Epoch 4/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.4940 - acc: 0.4443 - val_loss: 4.4153 - val_acc: 0.4497\n",
            "Epoch 5/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.4361 - acc: 0.4463 - val_loss: 4.3710 - val_acc: 0.4505\n",
            "Epoch 6/30\n",
            "270/270 [==============================] - 50s 183ms/step - loss: 4.3904 - acc: 0.4473 - val_loss: 4.3277 - val_acc: 0.4511\n",
            "Epoch 7/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.3444 - acc: 0.4484 - val_loss: 4.2877 - val_acc: 0.4518\n",
            "Epoch 8/30\n",
            "270/270 [==============================] - 50s 184ms/step - loss: 4.3009 - acc: 0.4496 - val_loss: 4.2511 - val_acc: 0.4522\n",
            "Epoch 9/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.2595 - acc: 0.4505 - val_loss: 4.2159 - val_acc: 0.4527\n",
            "Epoch 10/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.2220 - acc: 0.4513 - val_loss: 4.1836 - val_acc: 0.4538\n",
            "Epoch 11/30\n",
            "270/270 [==============================] - 50s 184ms/step - loss: 4.1864 - acc: 0.4522 - val_loss: 4.1623 - val_acc: 0.4533\n",
            "Epoch 12/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.1515 - acc: 0.4535 - val_loss: 4.1237 - val_acc: 0.4549\n",
            "Epoch 13/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.1173 - acc: 0.4543 - val_loss: 4.0925 - val_acc: 0.4560\n",
            "Epoch 14/30\n",
            "270/270 [==============================] - 50s 184ms/step - loss: 4.0841 - acc: 0.4550 - val_loss: 4.0663 - val_acc: 0.4567\n",
            "Epoch 15/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.0522 - acc: 0.4563 - val_loss: 4.0381 - val_acc: 0.4576\n",
            "Epoch 16/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 4.0219 - acc: 0.4574 - val_loss: 4.0114 - val_acc: 0.4591\n",
            "Epoch 17/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.9938 - acc: 0.4585 - val_loss: 3.9886 - val_acc: 0.4602\n",
            "Epoch 18/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.9676 - acc: 0.4595 - val_loss: 3.9700 - val_acc: 0.4610\n",
            "Epoch 19/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.9427 - acc: 0.4604 - val_loss: 3.9470 - val_acc: 0.4619\n",
            "Epoch 20/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.9180 - acc: 0.4615 - val_loss: 3.9269 - val_acc: 0.4627\n",
            "Epoch 21/30\n",
            "270/270 [==============================] - 50s 183ms/step - loss: 3.8942 - acc: 0.4625 - val_loss: 3.9072 - val_acc: 0.4635\n",
            "Epoch 22/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.8699 - acc: 0.4635 - val_loss: 3.8862 - val_acc: 0.4646\n",
            "Epoch 23/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.8442 - acc: 0.4648 - val_loss: 3.8650 - val_acc: 0.4652\n",
            "Epoch 24/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.8154 - acc: 0.4661 - val_loss: 3.8342 - val_acc: 0.4674\n",
            "Epoch 25/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.7859 - acc: 0.4675 - val_loss: 3.8101 - val_acc: 0.4680\n",
            "Epoch 26/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.7566 - acc: 0.4688 - val_loss: 3.7849 - val_acc: 0.4691\n",
            "Epoch 27/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.7283 - acc: 0.4701 - val_loss: 3.7622 - val_acc: 0.4701\n",
            "Epoch 28/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.7010 - acc: 0.4712 - val_loss: 3.7380 - val_acc: 0.4714\n",
            "Epoch 29/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.6735 - acc: 0.4726 - val_loss: 3.7151 - val_acc: 0.4727\n",
            "Epoch 30/30\n",
            "270/270 [==============================] - 49s 183ms/step - loss: 3.6467 - acc: 0.4738 - val_loss: 3.6949 - val_acc: 0.4731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzAPa6SBPUSA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GDxquIqNxHu"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwuLsxFTPVIb"
      },
      "source": [
        "\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
        "                      state_h, state_c])\n",
        "\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_art, latent_dim))\n",
        "\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKGYs2qPfER"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_len_summ-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p529q6xQcuF"
      },
      "source": [
        "\n",
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBkbDj8RQeTh",
        "outputId": "66d6936a-98f8-4dc6-9a3e-57a0689a6269"
      },
      "source": [
        "for i in range(0,5):\n",
        "  print(\"Article:\",seq2text(x_train[i]))\n",
        "  print(\"Original summary:\",seq2summary(y_train[i]))\n",
        "  print(\"Predicted summary:\",decode_sequence(x_train[i].reshape(1,max_len_art)))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article: swadeshi jagran manch economic wing rss said would seek legislative amendments rules governing patents came supreme court tuesday ruled us seed maker monsanto claim patents genetically modified cotton seeds india need forbid companies charging patent amount added \n",
            "Original summary: rss outfit seeks change in patent rules post monsanto verdict \n",
            "Predicted summary:  pm modi to be to be to be to be to be to be\n",
            "\n",
            "\n",
            "Article: retired australian spinner shane warne admitted ball century ashes fluke one deliveries leg spinners want bowl proud bowled warne said delivery warne first ashes pitched outside mike gatting leg stump spinning back hit stump \n",
            "Original summary: ball of the century was a admits shane warne \n",
            "Predicted summary:  kohli to be to be to be to be to be in india\n",
            "\n",
            "\n",
            "Article: elon musk released video boring company first tunnel los angeles us construction almost complete musk said tunneling company offering free rides public months notably tunnel enable mass transit pedestrians cyclists transported via pods avoid traffic \n",
            "Original summary: elon musk releases video of st boring company tunnel in us \n",
            "Predicted summary:  uber to buy crore in india in india\n",
            "\n",
            "\n",
            "Article: devotee dropped iphone donation box lord swamy temple andhra pradesh matter came light saturday staffers opened box count day offering devotees temple superintendent said per norms prescribed endowments department smartphones found donation box buried \n",
            "Original summary: devotee offers iphone s at temple in andhra pradesh \n",
            "Predicted summary:  govt to be to be to be to be to be to be\n",
            "\n",
            "\n",
            "Article: india publicly listed family owned businesses third highest world china ranked first companies according credit suisse report india ranked nd terms average market capitalisation family owned companies average value billion average market capitalisation family owned companies greatest spain billion \n",
            "Original summary: india has third highest number of family businesses report \n",
            "Predicted summary:  india to buy crore in india in india\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XLYWR3SQhN8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}